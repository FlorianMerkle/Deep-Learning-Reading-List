{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf62699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "from foolbox.attacks import LinfPGD, LinearSearchBlendedUniformNoiseAttack\n",
    "from foolbox import PyTorchModel\n",
    "#from load_models import load_mobilenet\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8422c6",
   "metadata": {},
   "source": [
    "# Load Imagenette Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a68bb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenette(path, bs=32):\n",
    "    train_transforms = torchvision.transforms.Compose([\n",
    "        #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        #torchvision.transforms.RandomHorizontalFlip(),\n",
    "        #torchvision.transforms.RandomRotation(20),\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    val_transforms= torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_path= path+'/train'\n",
    "    imagenette_train = torchvision.datasets.ImageFolder(\n",
    "        root=train_path,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "\n",
    "    val_path=path+'/val'\n",
    "    imagenette_val = torchvision.datasets.ImageFolder(\n",
    "        root=val_path,\n",
    "        transform=val_transforms\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(imagenette_train, num_workers=4,\n",
    "                                              batch_size=bs,\n",
    "                                              shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(imagenette_val, num_workers=4,\n",
    "                                              batch_size=bs,\n",
    "                                              shuffle=True)\n",
    "    return train_loader, val_loader, (imagenette_train, imagenette_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af35aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/florian/data/imagenette2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d983c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dl, val_dl,(train_ds, val_ds) = load_imagenette(PATH, 5000)\n",
    "#x_train, y_train = next(iter(train_dl))\n",
    "#x_train, y_train = x_train.numpy(), y_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5a1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl,(train_ds, val_ds) = load_imagenette(PATH, 32)\n",
    "\n",
    "dataloaders = {\n",
    "    'train':train_dl, \n",
    "    'validation':val_dl\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train':len(train_dl.dataset), \n",
    "    'validation':len(val_dl.dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a0fb1",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e12696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "\n",
    "class ImageNetNormalization(nn.Module):\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        super(ImageNetNormalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torchvision.transforms.functional.normalize(x, self.mean, self.std)\n",
    "    \n",
    "\n",
    "def load_resnet(MODEL_PATH):\n",
    "    base_resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "    num_ftrs_in = base_resnet.fc.in_features\n",
    "    num_ftrs_out = 10\n",
    "    base_resnet.fc = nn.Linear(num_ftrs_in, num_ftrs_out)\n",
    "    resnet = torch.nn.Sequential(\n",
    "        #ImageNetNormalization(),\n",
    "        base_resnet\n",
    "    )\n",
    "\n",
    "    state_dict = torch.load(MODEL_PATH)\n",
    "    resnet.load_state_dict(state_dict)\n",
    "    resnet = resnet\n",
    "    model = resnet\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_mobilenet(MODEL_PATH):\n",
    "\n",
    "    base_mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "    num_ftrs_in = base_mobilenet.classifier[0].in_features\n",
    "    num_ftrs_out = base_mobilenet.classifier[0].out_features\n",
    "    base_mobilenet.classifier[0] = nn.Linear(num_ftrs_in, num_ftrs_out)\n",
    "\n",
    "    num_ftrs_in = base_mobilenet.classifier[3].in_features\n",
    "    num_ftrs_out = 10\n",
    "    base_mobilenet.classifier[3] = nn.Linear(num_ftrs_in, num_ftrs_out)\n",
    "\n",
    "    mobilenet = torch.nn.Sequential(\n",
    "        ImageNetNormalization(),\n",
    "        base_mobilenet\n",
    "    )\n",
    "\n",
    "    \n",
    "    state_dict = torch.load(MODEL_PATH)\n",
    "    mobilenet.load_state_dict(state_dict)\n",
    "    mobilenet = mobilenet\n",
    "\n",
    "    model = mobilenet\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_mobilenet(pretrained=True):\n",
    "    model = models.mobilenet_v3_small(pretrained=pretrained)\n",
    "\n",
    "    num_ftrs_in = model.classifier[0].in_features\n",
    "    num_ftrs_out = model.classifier[0].out_features\n",
    "    model.classifier[0] = nn.Linear(num_ftrs_in, num_ftrs_out)\n",
    "\n",
    "    num_ftrs_in = model.classifier[3].in_features\n",
    "    num_ftrs_out = 10\n",
    "    model.classifier[3] = nn.Linear(num_ftrs_in, num_ftrs_out)\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        model\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bf613",
   "metadata": {},
   "source": [
    "## Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a06efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNET_PATH = '../ET-Adversarials/models/MobileNetV3Small-wo-normalization-layer.pt'\n",
    "MNET_PATH = '../ET-Adversarials/models/MobileNetV3Small.pt'\n",
    "model = load_mobilenet(MNET_PATH).to(device)\n",
    "#model = load_resnet(RNET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef48d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b667f841",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_763691/2006294996.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val'"
     ]
    }
   ],
   "source": [
    "norm = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "images, labels = next(iter(dataloaders['val']))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "model.eval()\n",
    "fmodel = PyTorchModel(model, bounds=(0,1), preprocessing=preprocessing)\n",
    "attack = LinfPGD(abs_stepsize=(2/255), steps=50, random_start=True)\n",
    "_, advs, success = attack(fmodel, images, labels, epsilons=8/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a9ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cb927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e725828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc1999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(model, dataloader, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    norm = torchvision.transforms.Normalize(mean, std)\n",
    "    model.eval()\n",
    "    running_corrects = 0.\n",
    "    running_loss = 0.\n",
    "    for images,labels in dataloader:\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        outputs = model(norm(images))\n",
    "        running_loss += criterion(outputs,labels).detach()\n",
    "        running_corrects += (outputs.argmax(-1)==labels).sum()\n",
    "    model.train()\n",
    "    return (running_loss/len(dataloader.dataset)).item(), (running_corrects/len(dataloader.dataset)).item()\n",
    "\n",
    "def evaluate_rob_acc(model, attack, eps, dataloader, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), fmodel_bounds=(0,1)):\n",
    "    norm = torchvision.transforms.Normalize(mean, std)\n",
    "    model.eval()\n",
    "    running_corrects = 0.\n",
    "    running_loss = 0.\n",
    "    fmodel = PyTorchModel(model, bounds=fmodel_bounds, preprocessing=dict(mean=mean, std=std, axis=-3)) \n",
    "    for images,labels in dataloader:\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        _,adv_images,_ = attack(fmodel, images, labels, epsilons=[8/255])\n",
    "        adv_outputs = model(norm(adv_images[0]))\n",
    "        running_loss += criterion(adv_outputs,labels).detach()\n",
    "        running_corrects += (adv_outputs.argmax(-1)==labels).sum()\n",
    "    model.train()\n",
    "    return (running_loss/len(dataloader.dataset)).item(), (running_corrects/len(dataloader.dataset)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1062a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.9666242599487305, 0.0)\n",
      "(0.0028967992402613163, 0.9714649319648743)\n"
     ]
    }
   ],
   "source": [
    "criterion =nn.CrossEntropyLoss()\n",
    "attack = LinfPGD(abs_stepsize=(2/255), steps=7, random_start=True)\n",
    "eps = [8/255]\n",
    "dataloader=dataloaders['validation']\n",
    "print(evaluate_rob_acc(model, LinfPGD(abs_stepsize=(2/255), steps=7, random_start=True), [8/255], dataloader, mean=(0,0,0), std=(1,1,1)))\n",
    "print(evaluate_acc(model, dataloader, mean=(0,0,0), std=(1,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae9328d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3593832506072447\n",
      "train  - clean acc -  (0.032010260969400406, 0.6618438959121704)\n",
      "train  - robust acc -  (0.09481500834226608, 0.20656880736351013)\n",
      "0 0.7301910828025477\n",
      "validation  - clean acc -  (0.026518234983086586, 0.7301910519599915)\n",
      "validation  - robust acc -  (0.08803560584783554, 0.25197452306747437)\n",
      "1 0.3731122610624142\n",
      "train  - clean acc -  (0.03362637385725975, 0.6644841432571411)\n",
      "train  - robust acc -  (0.057529378682374954, 0.365719735622406)\n",
      "1 0.7161783439490446\n",
      "validation  - clean acc -  (0.028730476275086403, 0.716178297996521)\n",
      "validation  - robust acc -  (0.05305912345647812, 0.414522260427475)\n",
      "2 0.40901890379131906\n",
      "train  - clean acc -  (0.031348083168268204, 0.7009187936782837)\n",
      "train  - robust acc -  (0.05385657772421837, 0.4186292290687561)\n",
      "2 0.7561783439490446\n",
      "validation  - clean acc -  (0.026478542014956474, 0.7561783194541931)\n",
      "validation  - robust acc -  (0.04898499324917793, 0.47363054752349854)\n",
      "3 0.43415355370155245\n",
      "train  - clean acc -  (0.029809804633259773, 0.7176048159599304)\n",
      "train  - robust acc -  (0.052504926919937134, 0.4296124279499054)\n",
      "3 0.7645859872611465\n",
      "validation  - clean acc -  (0.02507016621530056, 0.7645859718322754)\n",
      "validation  - robust acc -  (0.04679223522543907, 0.4901910424232483)\n",
      "4 0.4531629527933256\n",
      "train  - clean acc -  (0.027988703921437263, 0.725525438785553)\n",
      "train  - robust acc -  (0.049028947949409485, 0.46235084533691406)\n",
      "4 0.7757961783439491\n",
      "validation  - clean acc -  (0.023412488400936127, 0.775796115398407)\n",
      "validation  - robust acc -  (0.04414793848991394, 0.5179617404937744)\n",
      "5 0.4683704720667441\n",
      "train  - clean acc -  (0.02766679972410202, 0.716865599155426)\n",
      "train  - robust acc -  (0.04994093254208565, 0.4507339894771576)\n",
      "5 0.7661146496815286\n",
      "validation  - clean acc -  (0.02342822402715683, 0.7661145925521851)\n",
      "validation  - robust acc -  (0.04514266550540924, 0.5082802176475525)\n",
      "6 0.4816770514309853\n",
      "train  - clean acc -  (0.02584870345890522, 0.7556236386299133)\n",
      "train  - robust acc -  (0.04557503014802933, 0.4994191527366638)\n",
      "6 0.8007643312101911\n",
      "validation  - clean acc -  (0.021827464923262596, 0.800764262676239)\n",
      "validation  - robust acc -  (0.041239820420742035, 0.5546496510505676)\n",
      "7 0.4916041820678002\n",
      "train  - clean acc -  (0.02901037037372589, 0.7082057595252991)\n",
      "train  - robust acc -  (0.05728486180305481, 0.380293607711792)\n",
      "7 0.7658598726114649\n",
      "validation  - clean acc -  (0.024537142366170883, 0.765859842300415)\n",
      "validation  - robust acc -  (0.05232192203402519, 0.4242038130760193)\n",
      "8 0.5021649593410075\n",
      "train  - clean acc -  (0.024852825328707695, 0.7528778314590454)\n",
      "train  - robust acc -  (0.04689497873187065, 0.4861125946044922)\n",
      "8 0.8005095541401274\n",
      "validation  - clean acc -  (0.02092002145946026, 0.800509512424469)\n",
      "validation  - robust acc -  (0.042729634791612625, 0.5332483649253845)\n",
      "9 0.5082902101594677\n",
      "train  - clean acc -  (0.02467961236834526, 0.7778012752532959)\n",
      "train  - robust acc -  (0.04576067626476288, 0.4935051202774048)\n",
      "9 0.8198726114649681\n",
      "validation  - clean acc -  (0.02088719606399536, 0.8198725581169128)\n",
      "validation  - robust acc -  (0.041211869567632675, 0.5439490079879761)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "PGD7_attack() got an unexpected keyword argument 'epsilons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_763691/3986447375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m liveloss = fit_model(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_763691/3798112832.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, criterion, optimizer, dataloaders, device, num_epochs, advtrain_hook, mean, std)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' - robust acc - '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrob_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEST_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final robust accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluate_rob_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final clean accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_763691/141273627.py\u001b[0m in \u001b[0;36mevaluate_rob_acc\u001b[0;34m(model, attack, eps, dataloader, mean, std, fmodel_bounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madv_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0madv_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PGD7_attack() got an unexpected keyword argument 'epsilons'"
     ]
    }
   ],
   "source": [
    "model = load_mobilenet(MNET_PATH).to(device)\n",
    "\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataloaders = dataloaders\n",
    "num_epochs = 10\n",
    "save_name=None\n",
    "scheduler=None\n",
    "advtrain=True\n",
    "eps = [8]\n",
    "dataloader = dataloaders['validation']\n",
    "\n",
    "\n",
    "\n",
    "liveloss = fit_model(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    dataloaders, \n",
    "    device, \n",
    "    num_epochs,\n",
    "    mean=(0,0,0),\n",
    "    std=(1,1,1)\n",
    "    #advtrain_hook=None,\n",
    "    #mean=(0.485, 0.456, 0.406), \n",
    "    #std=(0.229, 0.224, 0.225)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('final robust accuracy: ',evaluate_rob_acc(model, LinfPGD(abs_stepsize=(2/255), steps=7, random_start=True), [8/255], dataloaders['validation'], mean=(0,0,0), std=(1,1,1)))\n",
    "print('final clean accuracy: ',evaluate_acc(model, dataloaders['validation'], mean=(0,0,0), std=(1,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7791bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '../ET-Adversarials/models/MobileNetV3Small-adversarially-trained.pt'\n",
    "\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c352bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD7_training():\n",
    "    # Madry adversarial training with PGD-7\n",
    "    attack = LinfPGD(abs_stepsize=(2/255), steps=7, random_start=True)\n",
    "    attack_kwargs = {\"epsilons\": (8/255)}\n",
    "    fmodel_bounds = (0,1)\n",
    "\n",
    "    def PGD7_attack(fmodel, images, labels):\n",
    "        return attack(fmodel, images, labels, **attack_kwargs)[1]\n",
    "    \n",
    "    return PGD7_attack, fmodel_bounds\n",
    "\n",
    "def fit_model(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    dataloaders, \n",
    "    device, \n",
    "    num_epochs,\n",
    "    advtrain_hook=PGD7_training, \n",
    "    mean=(0.485, 0.456, 0.406), \n",
    "    std=(0.229, 0.224, 0.225)\n",
    "):\n",
    "    BEST_MODEL_PATH = './tmp/best_model.pt'\n",
    "    begin = time()\n",
    "    model = model.to(device) # Moves and/or casts the parameters and buffers to device.\n",
    "    best_val_acc = 0\n",
    "    norm = torchvision.transforms.Normalize(mean, std)\n",
    "    if advtrain_hook is not None:\n",
    "        attack, fmodel_bounds = advtrain_hook()\n",
    "\n",
    "    for epoch in range(num_epochs): # Number of passes through the entire training & validation datasets\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']: # First train, then validate\n",
    "\n",
    "            # Switch between training and test eval mode depending on phase.\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "\n",
    "            running_loss = 0.0 # keep track of loss\n",
    "            running_corrects = 0 # count of carrectly classified images\n",
    "\n",
    "            for images, labels in dataloaders[phase]:\n",
    "                images = images.to(device) # Perform Tensor device conversion\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if phase == \"train\" and advtrain_hook is not None:\n",
    "                    # Perturb images before computing gradients\n",
    "                    model.eval()\n",
    "                    preprocessing = dict(mean=mean, std=std, axis=-3)\n",
    "                    fmodel = PyTorchModel(model, bounds=fmodel_bounds, preprocessing=preprocessing)\n",
    "                    images = attack(fmodel, images, labels)\n",
    "                    model.train()\n",
    "\n",
    "                # Compute gradients and update weights\n",
    "                outputs = model(norm(images))\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == \"train\":\n",
    "                    optimizer.zero_grad() # Set all previously calculated gradients to 0\n",
    "                    loss.backward() # Calculate gradients\n",
    "                    optimizer.step() # Update weights\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1) # Get model's predictions\n",
    "                running_loss += loss.detach() * images.size(0) # multiply mean loss by the number of elements\n",
    "                running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total\n",
    "\n",
    "            epoch_loss = running_loss.item() / len(dataloaders[phase].dataset) # get the \"mean\" loss for the epoch\n",
    "            epoch_acc = running_corrects.item() / len(dataloaders[phase].dataset) # Get proportion of correct predictions\n",
    "\n",
    "            print(epoch, epoch_acc)\n",
    "            #print(epoch_loss)\n",
    "\n",
    "            # Logging\n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss\n",
    "            logs[prefix + 'accuracy'] = epoch_acc\n",
    "\n",
    "            \n",
    "                \n",
    "            acc = evaluate_acc(model, dataloaders[phase], mean=mean, std=std)\n",
    "            rob_acc = evaluate_rob_acc(model, LinfPGD(abs_stepsize=(2/255), steps=7, random_start=True), [8/255], dataloaders[phase], mean=mean, std=std)\n",
    "            \n",
    "            if phase == 'validation' and acc[1]+rob_acc[1]>best_val_acc:\n",
    "                torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(phase, ' - clean acc - ', acc)\n",
    "            print(phase,' - robust acc - ', rob_acc)\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "    print('final robust accuracy: ',evaluate_rob_acc(model, LinfPGD(abs_stepsize=(2/255), steps=7, random_start=True), [8/255], dataloaders['validation'], mean=mean, std=std))\n",
    "    print('final clean accuracy: ',evaluate_acc(model, dataloaders['validation'], mean=mean, std=std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a639142",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = './tmp/best_model.pt'\n",
    "torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4963e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
